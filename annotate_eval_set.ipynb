{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_label_tuple_to_sentences_list = {}\n",
    "\n",
    "with open('eval_set.txt') as f:\n",
    "    verb_label_tuple = None\n",
    "    for line in f:\n",
    "        # skip if empty line, if only one space, split by space, otherwise take whole line\n",
    "        try:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            elif len(line.split()) == 2:\n",
    "                line_list = line.strip().split()\n",
    "                label = int(line_list[1])\n",
    "                verb = line_list[0]\n",
    "                verb_label_tuple = (verb, label)\n",
    "                verb_label_tuple_to_sentences_list[verb_label_tuple] = []\n",
    "            else:\n",
    "                verb_label_tuple_to_sentences_list[verb_label_tuple].append(line.strip())\n",
    "        except:\n",
    "            print(line)\n",
    "            raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_folder = None\n",
    "\n",
    "def get_annotated_sentences(verb, label):\n",
    "    verb_file = complete_data_folder + '/' + verb + '.csv'\n",
    "    sentences = verb_label_tuple_to_sentences_list[(verb, label)]\n",
    "    sentences_to_annotated_sentences = {}\n",
    "    with open(verb_file) as f:\n",
    "        for line in f:\n",
    "            if len(sentences) == 0:\n",
    "                raise Exception('No sentences for verb ' + verb + ' and label ' + str(label))\n",
    "            for sentence in sentences:\n",
    "                if sentence in line:\n",
    "                    sentences_to_annotated_sentences[sentence] = line.strip()\n",
    "            if len(sentences_to_annotated_sentences) == len(sentences):\n",
    "                return sentences_to_annotated_sentences\n",
    "            \n",
    "    print([sentence for sentence in sentences if sentence not in sentences_to_annotated_sentences])\n",
    "    raise Exception('Not enough sentences for verb ' + verb + ' and label ' + str(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adpositions = ['across', 'at', 'down', 'from', 'in', 'inside', 'into', 'near', 'off', 'off of', 'on', 'onto', 'out of', 'out to', 'outside', 'outside of', 'over', 'through', 'to', 'towards', 'under', 'up', 'within']\n",
    "\n",
    "def check_dobj(i, dependencies, verb):\n",
    "    if i >= len(dependencies):\n",
    "        return None\n",
    "    text, lemma, pos, dep, head, head_i = dependencies[i]\n",
    "\n",
    "    if pos in ['VERB', 'AUX']:\n",
    "        return None\n",
    "    \n",
    "    if dep == 'ROOT':\n",
    "        return None\n",
    "\n",
    "    if dep == 'dobj' and head == verb:\n",
    "        return i\n",
    "\n",
    "    return check_dobj(head_i, dependencies, verb) \n",
    "\n",
    "\n",
    "def check_adp(start_i, dependencies, adpositions): #note that this does not include dependencies because spacy can't be trusted on those\n",
    "    if start_i + 1 < len(dependencies):\n",
    "        longer_adp = dependencies[start_i][0] + \" \" + dependencies[start_i + 1][0]\n",
    "        if longer_adp in adpositions:\n",
    "            return ([start_i, start_i + 1], longer_adp)\n",
    "    \n",
    "    if start_i < len(dependencies) and dependencies[start_i][0] in adpositions:\n",
    "        return ([start_i], dependencies[start_i][1])\n",
    "\n",
    "    return (None, None)\n",
    "\n",
    "def check_pobj(dependencies, adp_index):\n",
    "    pobj_token_list = [(lemma, pobj_i) for pobj_i, (text, lemma, pos, dep, head, head_i) in enumerate(dependencies) if head_i in adp_index and dep == 'pobj' and pos not in ['NUM', 'PUNCT']]\n",
    "    if len(pobj_token_list) > 1:\n",
    "        raise Exception\n",
    "    if len(pobj_token_list) == 1:\n",
    "        return pobj_token_list[0]\n",
    "    return (None, None)\n",
    "\n",
    "# Why     why     SCONJ   advmod  is      1\n",
    "def is_cm(dependencies, nlp, sentence):\n",
    "    for i, (token_text, lemma, pos, dep, head, head_i) in enumerate(dependencies):\n",
    "        if pos == 'VERB':\n",
    "            # Look for a direct object (dobj) immediately following the verb\n",
    "            dobj_i = check_dobj(i + 1, dependencies, token_text)\n",
    "            if dobj_i:\n",
    "                dobj_lemma = dependencies[dobj_i][1]\n",
    "                # Check if the following token is an adposition (ADP) from the list\n",
    "                (adp_index, adp_lemma) = check_adp(dobj_i + 1, dependencies, adpositions)\n",
    "                if adp_index:\n",
    "                    # do another spacy dependency parse\n",
    "                    doc = nlp(sentence)\n",
    "                    dependencies = [(token.text, token.lemma_, token.pos_, token.dep_, token.head.text, token.head.i) for token in doc]\n",
    "                    pobj_lemma, pobj_i = check_pobj(dependencies, adp_index)\n",
    "                    if pobj_lemma:\n",
    "                        result_dict = {'verb': lemma, 'verb_i' : i, 'direct_object': dobj_lemma, 'direct_object_i': dobj_i, 'preposition': adp_lemma, 'preposition_i': adp_index, 'prepositional_object': pobj_lemma, 'prepositional_object_i': pobj_i}\n",
    "                        return result_dict\n",
    "                    # Look for a prepositional object (pobj) child of the adp\n",
    "\n",
    "\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def dependency_parsing(sentences):\n",
    "    parsed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        dependencies = [(token.text, token.lemma_, token.pos_, token.dep_, token.head.text, token.head.i) for token in doc]\n",
    "        parsed_sentences.append({\"sentence\": sentence, \"dependencies\": dependencies})\n",
    "    return parsed_sentences\n",
    "\n",
    "def get_annotated_sentences(verb, label):\n",
    "    sentences = verb_label_tuple_to_sentences_list[(verb, label)]\n",
    "    parsed_sentences = dependency_parsing(sentences)\n",
    "    sentences_to_annotated_sentences = {}\n",
    "    \n",
    "    for sentence, parsed_sentence in zip(sentences, parsed_sentences):\n",
    "        result = is_cm(parsed_sentence['dependencies'], nlp, sentence)\n",
    "        if result:\n",
    "            sentences_to_annotated_sentences[sentence] = f\"{sentence},{result['verb']},{result['direct_object']},{result['preposition']},{result['prepositional_object']},{result['verb_i']},{result['direct_object_i']},{result['preposition_i']},{result['prepositional_object_i']}\"\n",
    "        else:\n",
    "            print('No result for sentence: ' + sentence)\n",
    "            raise Exception\n",
    "    return sentences_to_annotated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:07<00:00, 14.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "out_file = \"annotated_eval_set.csv\"\n",
    "for verb_label_tuple in tqdm(verb_label_tuple_to_sentences_list):\n",
    "    verb = verb_label_tuple[0]\n",
    "    label = verb_label_tuple[1]\n",
    "    sentences_to_annotated_sentences = get_annotated_sentences(verb, label)\n",
    "    if not sentences_to_annotated_sentences:\n",
    "        raise Exception ('No sentences for verb ' + verb + ' and label ' + str(label))\n",
    "    with open(out_file, 'a') as f:\n",
    "        for sentence in sentences_to_annotated_sentences:\n",
    "            f.write(sentences_to_annotated_sentences[sentence] + \",\" + str(label) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
